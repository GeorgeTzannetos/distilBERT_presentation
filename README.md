# distilBERT_presentation
In this repository, I give an overview of BERT and examine the practical extensions of distilBERT and similar techniques, by going through the distilBERT paper.
Important take away points are:
 * the success of large models such as BERT, based on Transformers and fine-tuning, 
 * the need for compression techniques, that can reduce the computational and memory costs of these large models, whilst keeping the performance high. This can enable such models 
 to be adopted in production and deployed on various devices. 
# References

[1] https://arxiv.org/pdf/1910.01108.pdf

[2] https://arxiv.org/pdf/1810.04805.pdf
